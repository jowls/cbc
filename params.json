{"name":"CBC","tagline":"Web scraping CBC comments with Raspberry Pi","body":"#Web Scraping with Raspberry Pi\r\n\r\n##About\r\n\r\nA small project involving web scraping javascript content using: the raspberry pi, python, scrapy and selenium/firefox webdriver. Results are stored in postgresql using the sqlalchemy library. Statistics about the database can optionally be served over the web through a lightweight golang process.\r\n\r\nThe roadmap is to integrate the nltk library to perform basic sentiment analysis on news articles, and the comments they elicit.\r\n\r\nComing soon. You will see content being published in bits and pieces, but the site is still under heavy construction; please do not use this page as a tutorial yet. Contact me on github or at jowls@outlook.com to kick me in the.\r\n\r\n##Readme Contents\r\n\r\n* Dependencies\r\n* Setup Pi\r\n* Special Requirements\r\n* Code\r\n* Configuration\r\n* Golang Webserver (Optional)\r\n\r\n##Dependencies:\r\n\r\n* psycopg2\r\n* scrapy\r\n* sqlalchemy\r\n* pyvirtualdisplay\r\n* selenium\r\n* Xvfb\r\n\r\n##Setup Pi\r\n\r\n###Install raspbian wheezy on a new SD card and boot up Pi.\r\n\r\n###Install Dependencies\r\n\r\nInstall postgresql (9.1.9 at time of writing)\r\n\r\n    sudo apt-get install postgresql\r\n\r\nInstall firefox (iceweasel 10.0.12esr at time of writing) and pip\r\n\r\n    sudo apt-get install iceweasel python-pip\r\n\r\nInstall additional python dependencies using pip (scrapy 16.5, selenium 2.33.0 at time of writing)\r\n\r\n    sudo pip install -U scrapy selenium\r\n\r\n##Special Requirements\r\n\r\nBecause of the Pi's limited CPU/GPU, you should make the following adjustment to selenium's firefox webdriver code.\r\nsudo nano /usr/local/lib/python2.7/dist-packages/selenium/webdriver/firefox/firefox_binary.py\r\n\r\non line 91 in the wait_until_connectable function, change `count == 30` to be `count == 80` in order to give firefox some extra time to warm up.\r\n\r\n##Code\r\n\r\nClone the repository from GitHub\r\n\r\n    git clone https://github.com/jowls/cbc.git\r\n\r\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\r\n\r\n    Testing 1-2-3..\r\n\r\nUt enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum\r\nConfiguration\r\n\r\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum\r\n\r\n##Cron Job Configuration:\r\n\r\n\"scrapy crawl cbc\" will run the spider manually, or put this in /etc/crontab to schedule some crawls:\r\n\r\n\t17 * * * * pi /home/pi/cbc/crawl.sh >> /home/pi/cronwork.log 2>&1\r\n\t37 * * * * root sudo killall crawl.sh\r\n\t38 * * * * root sudo killall Xvfb\r\n\t39 * * * * root sudo killall iceweasel\r\n\r\n(The above runs at 17m past the hour - on raspbian wheezy)\r\n\r\n##Golang Webserver (Optional)\r\n\r\n####Install go\r\n\r\n####Build go code\r\n\r\n    go build web.go\r\n\r\n####Run go code\r\n\r\nIt must be run as sudo because it will be serving on port 80.\r\n\r\n    sudo ./web&\r\n\r\nCopyright jowls 2013\r\n","google":"UA-42365313-1","note":"Don't delete this file! It's used internally to help with page regeneration."}